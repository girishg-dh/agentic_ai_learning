{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b8c6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: <|channel|>analysis<|message|>User says Hello. Just respond politely.<|end|><|start|>assistant<|channel|>final<|message|>Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import readline # Enables input line editing\n",
    "from pathlib import Path\n",
    " \n",
    "import lmstudio as lms\n",
    " \n",
    "# Define a function that can be called by the model and provide them as tools to the model.\n",
    "# Tools are just regular Python functions. They can be anything at all.\n",
    "def create_file(name: str, content: str):\n",
    "    \"\"\"Create a file with the given name and content.\"\"\"\n",
    "    dest_path = Path(name)\n",
    "    if dest_path.exists():\n",
    "        return \"Error: File already exists.\"\n",
    "    try:\n",
    "        dest_path.write_text(content, encoding=\"utf-8\")\n",
    "    except Exception as exc:\n",
    "        return \"Error: {exc!r}\"\n",
    "    return \"File created.\"\n",
    " \n",
    "def print_fragment(fragment, round_index=0):\n",
    "    # .act() supplies the round index as the second parameter\n",
    "    # Setting a default value means the callback is also\n",
    "    # compatible with .complete() and .respond().\n",
    "    print(fragment.content, end=\"\", flush=True)\n",
    " \n",
    "model = lms.llm(\"openai/gpt-oss-20b\")\n",
    "chat = lms.Chat(\"You are a helpful assistant running on the user's computer.\")\n",
    " \n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User (leave blank to exit): \")\n",
    "    except EOFError:\n",
    "        print()\n",
    "        break\n",
    "    if not user_input:\n",
    "        break\n",
    "    chat.add_user_message(user_input)\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    model.act(\n",
    "        chat,\n",
    "        [create_file],\n",
    "        on_message=chat.append,\n",
    "        on_prediction_fragment=print_fragment,\n",
    "    )\n",
    "    print()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab2a692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**MXFP4 Quantization – A Quick Overview**\n",
      "\n",
      "> **MXFP4** stands for **Mixed‑Precision Floating‑Point 4‑bit** quantization.  \n",
      "> It’s a way of compressing deep‑learning models (especially those that run on edge devices or specialized hardware) by representing most of the network’s weights and activations with only **4 bits** of precision, while still retaining a small set of higher‑precision “anchor” parameters to keep accuracy high.\n",
      "\n",
      "---\n",
      "\n",
      "## Why Quantize at All?\n",
      "\n",
      "- **Memory Footprint:** Full 32‑bit floating‑point tensors can be huge. Reducing them to 4 bits cuts memory by **8×**.\n",
      "- **Compute Efficiency:** Many modern accelerators (e.g., Google’s Edge TPU, NVIDIA’s TensorRT, or custom ASICs) have fast integer‑only kernels that run much faster than floating‑point ones when data is in low precision.\n",
      "- **Energy Savings:** Less data movement and simpler arithmetic mean lower power consumption—critical for mobile/IoT devices.\n",
      "\n",
      "---\n",
      "\n",
      "## The “Mixed‑Precision” Twist\n",
      "\n",
      "Traditional 4‑bit quantization (often called *uniform* or *linear* quantization) applies the same scaling factor to all weights in a layer. That works well when the weight distribution is tight, but it can hurt performance if there are outliers or a wide dynamic range.\n",
      "\n",
      "MXFP4 addresses this by:\n",
      "\n",
      "1. **Partitioning Parameters:**\n",
      "   - **Base Set (≈ 95 % of parameters):** Stored as 4‑bit integers using a *per‑tensor* scaling factor.\n",
      "   - **Anchor Set (≈ 5 % of parameters):** Kept at higher precision (e.g., 8‑bit or even 16‑bit). These anchors capture the tails of the distribution and act like “control points” that help the network maintain its expressive power.\n",
      "\n",
      "2. **Dynamic Scaling:**\n",
      "   - For each tensor, a *global* scale factor is learned during fine‑tuning. The anchor parameters can also have their own per‑anchor scales if needed.\n",
      "   - During inference, weights are reconstructed by multiplying the 4‑bit integer values by the appropriate scale and adding any anchor corrections.\n",
      "\n",
      "3. **Training‑time Calibration:**\n",
      "   - A short *quantization‑aware training* (QAT) phase adjusts both the integer weights and the anchor set so that the network learns to compensate for quantization noise.\n",
      "   - The anchors are typically updated less frequently or with a smaller learning rate, ensuring stability.\n",
      "\n",
      "---\n",
      "\n",
      "## How It Works in Practice\n",
      "\n",
      "| Step | What Happens | Why |\n",
      "|------|--------------|-----|\n",
      "| **1. Initialize** | Start from a pre‑trained FP32 model. Split weights into base and anchor groups based on magnitude (e.g., top 5 % largest absolute values become anchors). | Keeps the most influential parameters in higher precision. |\n",
      "| **2. Quantize Base Weights** | Map each base weight to a 4‑bit integer using the per‑tensor scale: `q = round(w / s)` where `s` is chosen such that `q ∈ [0, 15]`. | Reduces storage and allows fast int8/int4 ops. |\n",
      "| **3. Reconstruct for Forward Pass** | Compute approximate weight: `w_hat = q * s + a_anchor` (where `a_anchor` is the corresponding anchor correction). | Provides a better approximation than raw integer weights alone. |\n",
      "| **4. QAT Loop** | Run forward‑backward passes, compute gradients w.r.t. both `q` and anchors, update them using Adam or SGD with small learning rates for anchors. | The network learns to minimize loss while dealing with low‑precision noise. |\n",
      "| **5. Export** | Store the 4‑bit integers, scale factors, and anchor values in a compact format (often packed into bytes). | Ready for deployment on hardware that supports int4 inference. |\n",
      "\n",
      "---\n",
      "\n",
      "## Benefits\n",
      "\n",
      "- **Higher Accuracy than Uniform 4‑bit:** The anchor set preserves critical information, often achieving FP32‑level accuracy with only a handful of high‑precision weights.\n",
      "- **Compact Model Size:** Even with anchors, the overall size shrinks dramatically (≈ 10–12× smaller than FP32).\n",
      "- **Hardware Friendly:** Works well on devices that support int4 kernels plus optional small integer‑to-float conversion for the anchors.\n",
      "\n",
      "---\n",
      "\n",
      "## Limitations & Considerations\n",
      "\n",
      "1. **Anchor Selection Overhead:** Determining which weights become anchors can add a preprocessing step; however, it’s usually negligible compared to training time.\n",
      "2. **Hardware Support Needed:** Not all accelerators have native int4 support; some may emulate it with int8 ops, slightly reducing the speed benefit.\n",
      "3. **Fine‑Tuning Required:** Pure post‑training quantization often fails to reach FP32 accuracy; QAT is usually necessary.\n",
      "\n",
      "---\n",
      "\n",
      "## Quick Example (Python Pseudocode)\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "# Assume `model` is a pre-trained nn.Module\n",
      "def mxfp4_quantize(model, anchor_ratio=0.05):\n",
      "    for name, param in model.named_parameters():\n",
      "        # 1) Flatten and sort by magnitude\n",
      "        flat = param.data.view(-1)\n",
      "        idx = torch.argsort(flat.abs(), descending=True)\n",
      "\n",
      "        n_anchor = int(anchor_ratio * len(flat))\n",
      "        anchor_idx = idx[:n_anchor]\n",
      "        base_idx   = idx[n_anchor:]\n",
      "\n",
      "        # 2) Create anchors (high‑precision copy)\n",
      "        anchors = flat[anchor_idx].clone()\n",
      "\n",
      "        # 3) Quantize base weights\n",
      "        base_vals = flat[base_idx]\n",
      "        s = base_vals.abs().max() / 7.0      # 4‑bit signed range [-8,7] -> scale to [−1,1]\n",
      "        q = torch.clamp(torch.round(base_vals / s), -8, 7).to(torch.int8)\n",
      "\n",
      "        # 4) Store\n",
      "        flat[anchor_idx] = anchors          # keep anchors as float\n",
      "        flat[base_idx]   = q                 # store quantized ints\n",
      "\n",
      "    return model\n",
      "```\n",
      "\n",
      "During inference you’d reconstruct the base weights by `q * s` and add back the anchors.\n",
      "\n",
      "---\n",
      "\n",
      "## Bottom Line\n",
      "\n",
      "**MXFP4 quantization is a smart compromise between aggressive 4‑bit compression and maintaining model fidelity.** By reserving a small, strategically chosen set of high‑precision “anchor” parameters, it keeps accuracy close to full precision while delivering the memory, speed, and energy benefits that make deep learning feasible on edge hardware.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"not-needed\"  # LM Studio does not require an API key\n",
    ")\n",
    " \n",
    "result = client.chat.completions.create(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"}\n",
    "    ]\n",
    ")\n",
    " \n",
    "print(result.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
